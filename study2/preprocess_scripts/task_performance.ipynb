{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, DataFrame\n",
    "\n",
    "NUM_PARTICIPANTS = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "def get_raw_data(part_id):\n",
    "    \n",
    "    my_file_dir = 'c:\\\\Users\\\\micha\\\\OneDrive\\\\My_GitHub_Repos\\\\robot-x-ar\\\\study2\\\\data\\\\raw_all_participants\\\\Participant_2-'+str(part_id)+'_1.csv'\n",
    "    big_df = read_csv(my_file_dir)\n",
    "    df = big_df[['ParticipantID', 'Condition', 'Trial', 'PostureID', 'Timestamp']]\n",
    "    df.columns = [\"pid\", \"condition\", \"trial\", \"posture\", \"timestamp\"]\n",
    "    \n",
    "    # print(\"\\n Finished reading raw csv file! \\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "##########################################################################################\n",
    "def calc_timestamp_diff(t1, t2):\n",
    "    # hours\n",
    "    h1 = int(str(t1)[8:10])\n",
    "    h2 = int(str(t2)[8:10])\n",
    "    hour_diff = h2 - h1\n",
    "    # minutes\n",
    "    m1 = int(str(t1)[10:12])\n",
    "    m2 = int(str(t2)[10:12])\n",
    "    min_diff = hour_diff*60 + (m2-m1)\n",
    "    # seconds\n",
    "    s1 = int(str(t1)[12:])/1e4\n",
    "    s2 = int(str(t2)[12:])/1e4\n",
    "    sec_diff = min_diff*60 + (s2-s1)\n",
    "    \n",
    "    return round(sec_diff, 4)\n",
    "    \n",
    "##########################################################################################\n",
    "def process_timestamp_lst(lst: list, start_idx: int):\n",
    "    diff_lst = []\n",
    "    for i in range(start_idx, len(lst)-1):\n",
    "        diff = calc_timestamp_diff(lst[i], lst[i+1])\n",
    "        diff_lst.append(diff)\n",
    "    return diff_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24]\n",
      "['yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes']\n",
      "['Q1FrontUp', 'Q1FrontDown', 'Q3FrontDown', 'Q3FrontUp', 'Q1FrontUp', 'Q1FrontDown', 'Q3FrontDown', 'Q3FrontUp', 'Q1FrontDown', 'Q3FrontUp', 'Q1FrontUp', 'Q3FrontDown', 'Q1FrontDown', 'Q3FrontUp', 'Q1FrontUp', 'Q3FrontDown', 'Q3FrontUp', 'Q3FrontDown', 'Q1FrontDown', 'Q1FrontUp', 'Q3FrontUp', 'Q3FrontDown', 'Q1FrontDown', 'Q1FrontUp', 'Q3FrontDown', 'Q1FrontUp', 'Q3FrontUp', 'Q1FrontDown', 'Q3FrontDown', 'Q1FrontUp', 'Q3FrontUp', 'Q1FrontDown', 'Q1FrontUp', 'Q1FrontDown', 'Q3FrontDown', 'Q3FrontUp', 'Q1FrontUp', 'Q1FrontDown', 'Q3FrontDown', 'Q3FrontUp', 'Q1FrontDown', 'Q3FrontUp', 'Q1FrontUp', 'Q3FrontDown', 'Q1FrontDown', 'Q3FrontUp', 'Q1FrontUp', 'Q3FrontDown', 'Q3FrontUp', 'Q3FrontDown', 'Q1FrontDown', 'Q1FrontUp', 'Q3FrontUp', 'Q3FrontDown', 'Q1FrontDown', 'Q1FrontUp', 'Q3FrontDown', 'Q1FrontUp', 'Q3FrontUp', 'Q1FrontDown', 'Q3FrontDown', 'Q1FrontUp', 'Q3FrontUp', 'Q1FrontDown', 'Q1FrontUp', 'Q1FrontDown', 'Q3FrontDown', 'Q3FrontUp', 'Q1FrontUp', 'Q1FrontDown', 'Q3FrontDown', 'Q3FrontUp', 'Q1FrontDown', 'Q3FrontUp', 'Q1FrontUp', 'Q3FrontDown', 'Q1FrontDown', 'Q3FrontUp', 'Q1FrontUp', 'Q3FrontDown', 'Q3FrontUp', 'Q3FrontDown', 'Q1FrontDown', 'Q1FrontUp', 'Q3FrontUp', 'Q3FrontDown', 'Q1FrontDown', 'Q1FrontUp', 'Q3FrontDown', 'Q1FrontUp', 'Q3FrontUp', 'Q1FrontDown', 'Q3FrontDown', 'Q1FrontUp', 'Q3FrontUp', 'Q1FrontDown', 'Q1FrontUp', 'Q1FrontDown', 'Q3FrontDown', 'Q3FrontUp', 'Q1FrontUp', 'Q1FrontDown', 'Q3FrontDown', 'Q3FrontUp', 'Q1FrontDown', 'Q3FrontUp', 'Q1FrontUp', 'Q3FrontDown', 'Q1FrontDown', 'Q3FrontUp', 'Q1FrontUp', 'Q3FrontDown', 'Q3FrontUp', 'Q3FrontDown', 'Q1FrontDown', 'Q1FrontUp', 'Q3FrontUp', 'Q3FrontDown', 'Q1FrontDown', 'Q1FrontUp', 'Q3FrontDown', 'Q1FrontUp', 'Q3FrontUp', 'Q1FrontDown', 'Q3FrontDown', 'Q1FrontUp', 'Q3FrontUp', 'Q1FrontDown', 'Q1FrontUp', 'Q1FrontDown', 'Q3FrontDown', 'Q3FrontUp', 'Q1FrontUp', 'Q1FrontDown', 'Q3FrontDown', 'Q3FrontUp', 'Q1FrontDown', 'Q3FrontUp', 'Q1FrontUp', 'Q3FrontDown', 'Q1FrontDown', 'Q3FrontUp', 'Q1FrontUp', 'Q3FrontDown', 'Q3FrontUp', 'Q3FrontDown', 'Q1FrontDown', 'Q1FrontUp', 'Q3FrontUp', 'Q3FrontDown', 'Q1FrontDown', 'Q1FrontUp', 'Q3FrontDown', 'Q1FrontUp', 'Q3FrontUp', 'Q1FrontDown', 'Q3FrontDown', 'Q1FrontUp', 'Q3FrontUp', 'Q1FrontDown', 'Q1FrontUp', 'Q1FrontDown', 'Q3FrontDown', 'Q3FrontUp', 'Q1FrontUp', 'Q1FrontDown', 'Q3FrontDown', 'Q3FrontUp', 'Q1FrontDown', 'Q3FrontUp', 'Q1FrontUp', 'Q3FrontDown', 'Q1FrontDown', 'Q3FrontUp', 'Q1FrontUp', 'Q3FrontDown', 'Q3FrontUp', 'Q3FrontDown', 'Q1FrontDown', 'Q1FrontUp', 'Q3FrontUp', 'Q3FrontDown', 'Q1FrontDown', 'Q1FrontUp', 'Q3FrontDown', 'Q1FrontUp', 'Q3FrontUp', 'Q1FrontDown', 'Q3FrontDown', 'Q1FrontUp', 'Q3FrontUp', 'Q1FrontDown']\n",
      "[44.1729, 25.3698, 10.0813, 9.693, 22.7206, 9.4913, 6.582, 13.4058, 11.354, 9.2924, 6.9722, 9.0104, 11.2788, 10.7759, 73.342, 10.3281, 128.7529, 81.0446, 134.3015, 165.4686, 22.6548, 28.4394, 58.4554, 11.6925, 16.4658, 154.1275, 18.5192, 118.1582, 102.9048, 23.3856, 15.7876, 46.8386, 64.79, 48.5386, 9.6583, 29.5925, 7.9738, 35.6042, 24.4687, 17.6249, 178.8129, 19.129, 18.7189, 34.9526, 43.5938, 23.9784, 37.4151, 23.8984, 46.716, 35.5893, 35.906, 93.9874, 43.8462, 13.1993, 78.2727, 64.9472, 11.821, 9.1529, 10.7487, 26.8918, 17.8664, 46.3186, 6.511, 36.0931, 37.5082, 85.178, 23.8924, 135.3879, 19.6093, 38.3091, 17.7957, 56.1263, 92.7573, 130.9352, 60.9561, 24.1265, 95.8885, 20.7576, 26.3036, 18.3285, 13.9872, 11.8347, 57.8469, 29.8214, 20.1237, 16.4611, 43.3816, 15.6474, 28.0017, 92.6075, 15.9675, 44.5319, 15.1044, 33.1124, 27.3997, 87.7886, 35.0809, 111.2186, 16.2898, 36.26, 23.0182, 90.7335, 12.7795, 22.7057, 70.8844, 27.3393, 67.6106, 35.8627, 28.9254, 39.9033, 30.4701, 38.8103, 34.0362, 62.7569, 10.5065, 31.9728, 10.9387, 11.6944, 34.1075, 29.3053, 102.7018, 175.7583, 59.5399, 143.3817, 26.5101, 45.4414, 29.341, 38.8439, 76.3007, 201.2916, 32.3912, 49.9045, 33.3044, 40.2884, 34.9441, 26.9953, 167.8209, 9.2426, 90.7789, 19.2417, 23.3971, 7.4567, 45.7314, 12.9668, 83.1994, 22.6266, 66.7743, 66.0254, 22.7204, 18.0591, 33.3117, 24.9819, 41.8819, 158.3085, 191.1275, 150.6748, 34.3353, 119.1647, 66.8299, 70.5037, 193.1155, 69.5063, 54.0435, 63.1448, 52.5744, 34.256, 15.856, 72.6418, 87.2487, 27.6031, 25.3416, 36.38, 38.5849, 25.2018, 77.119, 39.0954, 85.4327, 49.0397, 47.6185, 20.3295, 29.8529, 41.4956, 33.9002, 28.143, 12.3735, 37.2068, 30.678, 22.0228, 12.2572, 25.0762, 11.4764, 42.5954]\n",
      " Successfully written pre-processed data to csv file! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pid_list = []\n",
    "cond_list = []\n",
    "# trial_list = []\n",
    "posture_list = []\n",
    "time_list = []\n",
    "\n",
    "# loop through all participants\n",
    "for part_id in range(1, NUM_PARTICIPANTS+1):\n",
    "\n",
    "    task_df = get_raw_data(part_id)\n",
    "    # task_df\n",
    "    for i in range(1, 9):\n",
    "        tdf = task_df[task_df[\"trial\"]==i]\n",
    "        this_condition = tdf['condition'].tolist()[0]\n",
    "        if this_condition == \"C_HumanVertical\":\n",
    "            this_condition = \"yes\"\n",
    "        else:\n",
    "            this_condition = \"no\"\n",
    "        this_trial = i\n",
    "        this_posture = tdf['posture'].tolist()[0]\n",
    "        timestamp_list = tdf['timestamp'].tolist()\n",
    "        this_time = calc_timestamp_diff(timestamp_list[0], timestamp_list[-1])\n",
    "        \n",
    "        # add to lists\n",
    "        pid_list.append(part_id)\n",
    "        cond_list.append(this_condition)\n",
    "        # trial_list.append(this_trial)\n",
    "        posture_list.append(this_posture)\n",
    "        time_list.append(this_time)\n",
    "        \n",
    "\n",
    "# display the lists\n",
    "print(pid_list)\n",
    "print(cond_list)\n",
    "# print(trial_list)\n",
    "print(posture_list)\n",
    "print(time_list)\n",
    "\n",
    "assert len(pid_list)==len(cond_list)==len(posture_list)==len(time_list)\n",
    "\n",
    "###### write to new (cleaned) dataframe ######\n",
    "    \n",
    "# generate new dataframe\n",
    "df_dict = {\n",
    "    'pid': pid_list,\n",
    "    'condition': cond_list,\n",
    "    'posture': posture_list,\n",
    "    'mt': time_list\n",
    "}\n",
    "cleaned_df = DataFrame(df_dict)\n",
    "\n",
    "# write new dataframe to csv file\n",
    "dest_path = 'c:\\\\Users\\\\micha\\\\OneDrive\\\\My_GitHub_Repos\\\\robot-x-ar\\\\study2\\\\data\\\\task_performance\\\\movement_time_all.csv'\n",
    "cleaned_df.to_csv(dest_path, index=False)\n",
    "\n",
    "print(\" Successfully written pre-processed data to csv file! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21, 22, 22, 23, 23, 24, 24]\n",
      "['yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no']\n",
      "[89.317, 52.2, 36.629, 105.725, 509.568, 121.242, 307.271, 188.917, 152.579, 85.672, 251.613, 128.886, 212.199, 200.265, 58.614, 106.789, 281.966, 131.84, 308.775, 161.278, 113.49, 95.614, 181.109, 163.405, 149.237, 198.849, 138.109, 201.697, 86.046, 139.272, 140.136, 481.382, 135.532, 359.888, 89.552, 287.084, 99.073, 238.626, 290.834, 541.993, 175.328, 379.81, 180.001, 176.573, 133.392, 202.42, 91.405, 102.281]\n",
      " Successfully written pre-processed data to csv file! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pid_list = []\n",
    "cond_list = []\n",
    "time_list = []\n",
    "\n",
    "# loop through all participants\n",
    "for part_id in range(1, NUM_PARTICIPANTS+1):\n",
    "    this_part_df = cleaned_df[cleaned_df[\"pid\"]==part_id]\n",
    "    # print(this_part_df)\n",
    "    \n",
    "    # \"yes\" condition\n",
    "    yes_df = this_part_df[this_part_df['condition']==\"yes\"]\n",
    "    yes_time = round(sum(yes_df['mt'].tolist()), 3)\n",
    "    pid_list.append(part_id)\n",
    "    cond_list.append(\"yes\")\n",
    "    time_list.append(yes_time)\n",
    "    \n",
    "    # \"no\" condition\n",
    "    no_df = this_part_df[this_part_df['condition']==\"no\"]\n",
    "    no_time = round(sum(no_df['mt'].tolist()), 3)\n",
    "    pid_list.append(part_id)\n",
    "    cond_list.append(\"no\")\n",
    "    time_list.append(no_time)\n",
    "    \n",
    "print(pid_list)\n",
    "print(cond_list)\n",
    "print(time_list)\n",
    "\n",
    "###### write to new (cleaned) dataframe ######\n",
    "    \n",
    "# generate new dataframe\n",
    "df_dict = {\n",
    "    'pid': pid_list,\n",
    "    'condition': cond_list,\n",
    "    'time': time_list\n",
    "}\n",
    "cleaned_df = DataFrame(df_dict)\n",
    "\n",
    "# write new dataframe to csv file\n",
    "dest_path = 'c:\\\\Users\\\\micha\\\\OneDrive\\\\My_GitHub_Repos\\\\robot-x-ar\\\\study2\\\\data\\\\task_performance\\\\movement_time.csv'\n",
    "cleaned_df.to_csv(dest_path, index=False)\n",
    "\n",
    "print(\" Successfully written pre-processed data to csv file! \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
